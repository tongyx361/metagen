# Evaluation suite for reasoning used in "Demystifying Long CoT"
defaults:
  - /stages/qa@eval_reasoning_cot_boxed_qa_sampling.pipelines.cot_boxed_qa.stages.cot_boxed_qa
  - /datasets/aime_2024@eval_reasoning_cot_boxed_qa_sampling.datasets.aime_2024
  - /stages/qa@eval_reasoning_cot_boxed_qa_greedy.pipelines.cot_boxed_qa.stages.cot_boxed_qa
  - /datasets/math_500@eval_reasoning_cot_boxed_qa_greedy.datasets.math_500
  - /datasets/theorem_qa@eval_reasoning_cot_boxed_qa_greedy.datasets.theorem_qa
  - /datasets/mmlu_pro_1k@eval_reasoning_mmlu_pro_official_qa_greedy.datasets.mmlu_pro_1k
  - /stages/qa@eval_reasoning_mmlu_pro_official_qa_greedy.pipelines.mmlu_pro_official_qa.stages.mmlu_pro_official_qa

eval_reasoning_cot_boxed_qa_sampling:
  seeds: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
  pipelines:
    cot_boxed_qa:
      stages:
        cot_boxed_qa:
          messages: # sample -> messages
            # No system message
            question:
              role: user
              content_template:
                "{{ sample.problem | trim }}\n\nSolve the problem step by step
                and provide the final answer in \"\\boxed{...}\"."
          api_request: # client.text_completion(prompt=processed_text_seq, model=model) -> text_resp
            gen_params:
              max_tokens: 16384
              temperature: 0.7
              top_p: 0.95
eval_reasoning_cot_boxed_qa_greedy:
  seeds: [0]
  pipelines:
    cot_boxed_qa:
      stages:
        cot_boxed_qa:
          messages: # sample -> messages
            # No system message
            question:
              role: user
              content_template:
                "{{ sample.problem | trim }}\n\nSolve the problem step by step
                and provide the final answer in \"\\boxed{...}\"."
          api_request: # client.text_completion(prompt=processed_text_seq, model=model) -> text_resp
            gen_params:
              max_tokens: 16384
              temperature: 0.0
              top_p: 1.0
eval_reasoning_mmlu_pro_official_qa_greedy:
  seeds: [0]
  pipelines:
    mmlu_pro_official_qa:
      stages:
        mmlu_pro_official_qa:
          messages: # sample -> messages
            # No system message
            question:
              role: user
              # c.f. https://github.com/TIGER-AI-Lab/MMLU-Pro/blob/ef9890a4db3fd2c4fc50342606aac21093edb03f/evaluate_from_api.py#L216-L241
              content_template: 'The following are multiple choice questions
                (with answers) about {{ sample.category }}. Think step by step
                and then output the answer in the format of "The answer is (X)"
                at the end.


                Question: {{ sample.problem }}

                Options: {% for option_text in sample.option_texts %}{{ ["A",
                "B", "C", "D", "E", "F", "G", "H", "I", "J"][loop.index0] }}. {{
                option_text }}

                {% endfor %}'
          api_request: # client.text_completion(prompt=processed_text_seq, model=model) -> text_resp
            gen_params:
              max_tokens: 16384
              temperature: 0.0
              top_p: 1.0
