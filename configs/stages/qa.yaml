tokenization: # apply_chat_template(tokenizer=False) -> text_seq (containing model-specific tokens)
  add_generation_prompt: true
  continue_final_message: false
text_seq_preprocessing: # text_seq -> processed_text_seq
  template: "{{ text_seq }}" # Keep as is but also allow for custom prefixes
new_msg_preprocessing: # text_api_resp -> new_msg
  role: assistant
  content_template: "{{ text_api_resp.choices[0].text }}"
